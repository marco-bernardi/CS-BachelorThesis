\chapter{Design and coding}\label{cap:design-coding}
\intro{This chapter covers the project's design and coding, including the technologies and tools used, the software life cycle, and the coding phase.}

\section{Technology and tools}\label{sec:technology-tools}
In the subsequent sections, we will elucidate the technologies and tools employed within the project.
\subsection{Python}\label{subsec:python}
Python is a high-level programming language renowned for its simplicity, readability, and versatility. 
It offers a clean and concise syntax, making it easy to understand and write code. 
Python's extensive standard library and thriving community contribute to its vast ecosystem of third-party libraries and frameworks, empowering developers to accomplish a wide range of tasks efficiently. 
From web development and scientific computing to data analysis and machine learning, Python excels in various domains. 
Its object-oriented nature, dynamic typing, and automatic memory management contribute to its flexibility and ease of use. Furthermore, Python's cross-platform compatibility enables code portability across different operating systems. 
With its consistent updates and improvements, Python continues to evolve, ensuring its relevance in the ever-changing landscape of software development.

%Python is a programming language that lets you work quickly and integrate
%systems more effectively. It is a high-level programming language, with
%applications in numerous areas, including web programming, scripting, scientific
%computing, and artificial intelligence. It is very popular and used by many
%companies, such as Google, Facebook, Instagram, Spotify, Netflix, Dropbox, and
%many others. It is also used in the development of many open source projects,
%such as Blender, GIMP, Inkscape, and many others. It is a very versatile
%language, with a very large community and a very large number of libraries
%available. It is also a very simple language to learn, with a very simple
%syntax, which makes it very readable and easy to understand. It is also a
%language that is constantly evolving, with new versions released every year,
%which makes it always up to date and with the latest technologies.

\subsection{CUDA}\label{subsec:cuda}
\gls{CUDAG}\glsfirstoccur (Compute Unified Device Architecture) is a parallel computing platform and API model developed by NVIDIA. 
It allows developers to harness the computational power of NVIDIA \gls{GPUG}s\glsfirstoccur for general-purpose computing. 
By utilizing \gls{CUDAG}, developers can offload computationally intensive tasks to the \gls{GPUG}, resulting in significant performance improvements. 
\gls{CUDAG} provides a wide range of libraries and tools for efficient \gls{GPUG} programming, making it a standard for \gls{GPUG} computing in various industries and research fields. 
With ongoing advancements, \gls{CUDAG} continues to empower developers to leverage \gls{GPUG} parallelism for faster and more efficient computations.

\subsection{CuDNN}\label{subsec:cudnn}
\gls{CuDNNG}\glsfirstoccur (CUDA Deep Neural Network) is a \gls{GPUG}-accelerated library developed by NVIDIA for deep learning tasks. 
It provides optimized implementations of key neural network operations, such as convolutions and recurrent operations. 
By leveraging \gls{CuDNNG}, developers can accelerate deep learning workflows, reducing training times and improving model performance. 
It seamlessly integrates with popular deep learning frameworks and supports mixed-precision training. 
CuDNN plays a crucial role in advancing deep learning research and applications by harnessing the power of \gls{GPUG} acceleration.

\subsection{ML libraries}\label{subsec:mllib}
\subsubsection*{TensorFlow}\label{subsubsec:tensorflow}
TensorFlow is an open-source machine learning framework developed by Google. It provides a robust platform for building and deploying machine learning models. 
With its computational graph abstraction, TensorFlow enables efficient execution of complex mathematical computations. 
It offers a wide range of APIs and tools for different tasks, including model development, training, and deployment. 
TensorFlow supports distributed computing, allowing for parallel processing across multiple devices. It integrates with various frameworks and libraries, making it versatile and interoperable. 
TensorFlow is highly regarded for its scalability, flexibility, and extensive feature set, making it a popular choice among machine learning practitioners.
\subsubsection{PyTorch}\label{subsubsec:pytorch}
PyTorch is an open source machine learning library based on the Torch library,
used for applications such as computer vision and natural language processing,
primarily developed by Facebook's AI Research lab (FAIR). It is free and open
source software released under the Modified BSD license.
PyTorch serves as the foundation for several deep learning software applications, including Tesla Autopilot, Uber's Pyro, Hugging Face's Transformers, PyTorch Lightning, and Catalyst. 
It offers two prominent high-level capabilities: tensor computing, akin to NumPy, with enhanced acceleration through \gls{GPUG} utilization, and deep neural networks constructed on a tape-based automatic differentiation system. 
These features enable efficient computation and facilitate the development of sophisticated machine learning models.
\subsubsection{Keras}\label{subsubsec:keras}
Keras is a sophisticated, Python-based high-level neural networks \gls{apig} that can be seamlessly integrated with popular deep learning framework such as TensorFlow. 
It is specifically designed to facilitate rapid experimentation, allowing researchers and developers to quickly transition from conceptualizing ideas to obtaining meaningful results. 
The framework's core principle is to minimize the time lag between ideation and outcome, thereby facilitating efficient and effective research endeavors.
Prominent scientific organizations worldwide, including CERN, NASA, NIH, and others, rely on Keras for their research and applications. 
\subsubsection{OpenCV}\label{subsubsec:opencv}
OpenCV, short for Open Source Computer Vision, is a popular and widely-used open-source computer vision library. 
It was initially developed by Intel in 1999 and later supported by Willow Garage and Itseez (now merged with Intel) before becoming a community-driven project. 
OpenCV provides a vast collection of computer vision algorithms and tools, making it a go-to solution for developers and researchers working on various vision-related tasks.

The primary goal of OpenCV is to provide a comprehensive and efficient infrastructure for computer vision applications. 
It supports a wide range of programming languages, including C++, Python, Java, and MATLAB, making it accessible to developers from diverse backgrounds. 
OpenCV offers a rich set of functions for image and video processing, feature detection and extraction, object recognition, camera calibration, and more.

One of the key strengths of OpenCV is its ability to leverage hardware acceleration, such as utilizing multicore CPUs and GPUs, to enhance performance. 
This makes it suitable for real-time and resource-intensive applications, such as robotics, augmented reality, surveillance systems, and autonomous vehicles.

The library encompasses a wide range of functionalities, including face detection and recognition, object identification, human action classification in videos, camera movement tracking, object motion tracking, 3D model extraction, generation of 3D point clouds from stereo cameras, image stitching for creating high-resolution panoramic images, similarity search in image databases, red-eye removal from flash photography, eye movement tracking, scene recognition, and marker establishment for augmented reality overlays, among others.

\subsection{GANs Models}\label{subsec:gans-models}
\subsubsection{Pix2Pix}\label{subsubsec:pix2pix}
Pix2Pix\footcite{paper:pix2pix} is a popular deep learning model used for image-to-image translation tasks. It is based on a conditional generative adversarial network (\gls{gang}) architecture, which consists of a generator network and a discriminator network. 
The Pix2Pix model aims to learn a mapping between an input image and an output image, where the output image is a transformed version of the input image according to a specific target domain.

The model was trained and evaluated on a large dataset of paired images from the \textit{Berkeley Segmentation Dataset and Benchmark} and demonstrates a capability to generate plausible synthetic images for a variety of image-to-image translation tasks, such as converting daylight images to night.

Pix2Pix has been successfully applied to various image translation tasks, such as converting grayscale images to color, generating realistic street scenes from semantic labels, transforming sketches into photorealistic images, and more.
%\subsubsection*{ESRGAN}\label{subsubsec:esrgan}
%ESRGAN\footcite{paper:esrgan} is an enhanced version of the \gls{esrgang} model, which is a Generative
%Adversarial Network, or \gls{gang}, model designed for general purpose
%image-to-image translation. The model was trained and evaluated on a large
%dataset of paired images from the DIV2K dataset and demonstrates a capability to
%generate plausible synthetic images for a variety of image-to-image translation
%tasks, such as converting low resolution images to high resolution.

\subsubsection{StyleGAN}\label{subsubsec:stylegan}
StyleGAN is a highly versatile Generative Adversarial Network (\gls{gang}) model primarily developed for the purpose of general image generation. 
Extensive training and evaluation of the model were conducted using a substantial dataset comprised of unpaired images sourced from the Flickr-Faces-HQ dataset. Through this rigorous training process, StyleGAN exhibits the ability to generate synthetic images that possess a remarkable level of plausibility across a wide range of image generation tasks. 
Notably, the model excels in the generation of realistic human faces, showcasing its proficiency in capturing the intricate details and characteristics associated with facial features.

\subsection{Tools}\label{subsec:tools}
\subsubsection*{Visual Studio Code}\label{subsubsec:vscode}
Visual Studio Code is a free source-code editor made by Microsoft for Windows,
Linux and macOS.\@ Features include support for debugging, syntax highlighting,
intelligent code completion, snippets, code refactoring, and embedded Git.
\subsubsection{Git}\label{subsubsec:git}
Git is a distributed version-control system for tracking changes in source code
during software development. It is designed for coordinating work among
programmers, but it can be used to track changes in any set of files. Its goals
include speed, data integrity, and support for distributed, non-linear
workflows.
\subsubsection{GitHub}\label{subsubsec:github}
GitHub is a global company that provides hosting for software development
version control using Git. It is a subsidiary of Microsoft, which acquired the
company in 2018 for \$7.5 billion. It offers all of the distributed version
control and source code management (SCM) functionality of Git as well as adding
its own features. It provides access control and several collaboration features
such as bug tracking, feature requests, task management, and wikis for every
project.
\subsubsection{GIMP}\label{subsubsec:gimp}
GIMP is a free and open-source raster graphics editor used for image retouching
and editing, free-form drawing, converting between different image formats, and
more specialized tasks.
\subsection{Adobe Photoshop}\label{subsec:adobe-photoshop}
Adobe Photoshop, a widely recognized raster graphics editor, has been developed and published by Adobe Inc. for the Windows and macOS platforms. 
Since its inception in 1988 by Thomas and John Knoll, Photoshop has established itself as the industry standard for raster graphics editing and digital art creation. 
In a recent update, Adobe introduced a notable feature called "Generative Fill". 
This feature incorporates a Generative AI model, enabling users to generate new content within an image, thereby expanding the creative possibilities and augmenting the editing capabilities of the software.
\subsubsection{Webex}\label{subsubsec:webex}
Webex is a video conferencing software developed by Cisco Systems. It is a
cloud-based software that provides video conferencing, online meetings,
screen-sharing, and webinars. It has a free version that allows up to 100
participants, with a 50-minute time restriction. The paid version starts at
\$13.50/month and allows up to 200 participants and unlimited meeting time.
\subsubsection{Outlook}\label{subsubsec:outlook}
Outlook is a personal information manager software system from Microsoft,
available as a part of the Microsoft Office suite. Primarily an email
application, it also includes a calendar, task manager, contact manager,
note taking, journal, and web browsing.

\subsection{Hardware}\label{subsec:hardware}
The company provided all the necessary hardware for the development of this project. 
The hardware configuration utilized during the project is outlined as follows: \\
\begin{itemize}
    \item \textbf{DELL Precision 7670}
    \begin{itemize}
        \item \textbf{CPU}: Intel Core i7-12850HX
        \item \textbf{\gls{GPUG}}: NVIDIA Quadro RTX A2000 8GB GDDR6
        \item \textbf{RAM}: 32GB DDR4
        \item \textbf{Storage}: 512GB NVMe SSD
        \item \textbf{OS}: Windows 10 Pro 64-bit
    \end{itemize}
    \item \textbf{DELL Precision 7520}
    \begin{itemize}
        \item \textbf{CPU}: Intel Core i7-6820HQ
        \item \textbf{\gls{GPUG}}: NVIDIA Quadro M2200 4GB GDDR5
        \item \textbf{RAM}: 16GB DDR4
        \item \textbf{Storage}: 512GB NVMe SSD
        \item \textbf{OS}: Windows 10 Pro 64-bit
    \end{itemize}
\end{itemize}
\section{Final implementation}\label{sec:final-implementation}
\subsection{Network Type}\label{subsec:network-type}
In the ultimate implementation of the project, the selected network type was the Pix2Pix model.
\subsubsection{Pix2Pix in detail}\label{subsubsec:pix2pix-in-detail}
Pix2Pix is a comprehensive Generative Adversarial Network (\gls{cgang}) model specifically designed for the purpose of image-to-image translation. 
The Pix2Pix model comprises two distinct models, which are constructed as follows:
\subsubsection{U-Net Generator}
The generator follow the U-Net architecture, which is a convolutional neural network that
consists of an encoder (down-sampler) and a decoder (up-sampler). The encoder downsamples the
input image and extracts the features, while the decoder upsamples the image and produces
the segmentation map. The skip connections between the encoder and decoder are added to
prevent the loss of low-level features during the upsampling process.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{model/unet-gen}
    \caption{U-Net architecture}\label{fig:unet}
\end{figure}
\subsubsection{TensorFlow implementation}
The TensorFlow implementation of the U-Net generator is composed by the following layers (see figure~\ref{fig:gen-layer}):
\begin{itemize}
    \item \textbf{Encoder}: 8 downsampling layers, each downsampling layer is composed by a convolutional layer, a batch normalization layer, and a Leaky ReLU activation layer.
    \item \textbf{Decoder}: 8 upsampling layers, each upsampling layer is composed by a transposed convolutional layer, a batch normalization layer, a dropout layer(applied to the first 3 layers), and a ReLU activation layer.
    \item \textbf{Skip connections}: between the encoder and decoder, there are skip connections, each skip connection.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{model/generator-layer}
    \caption{TensorFlow implementation of the U-Net generator}\label{fig:gen-layer}
\end{figure}
\subsection{PatchGAN Discriminator}\label{subsec:patchgan-discriminator}
The discriminator is a convolutional neural network that classifies the real and fake
images. The discriminator architecture is such that each convolutional block in the
discriminator consists of a convolution layer, a batch normalization layer, and a Leaky
ReLU activation layer. The PatchGAN discriminator architecture is such that it only penalizes
the structure at the scale of patches. This discriminator tries to classify if each N x N
patch in an image is real or fake.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{model/patch-gan}
    \caption{PatchGAN discriminator.}\label{fig:patchgan}
\end{figure}
Each value of the output matrix in fig.~\ref{fig:patchgan} represents the probability of whether the corresponding image patch is real or it is artificially generated.
\subsubsection{TensorFlow implementation}
The TensorFlow implementation of the PatchGAN discriminator is composed by the following layers (see figure~\ref{fig:dis-layer}):
\begin{itemize}
    \item \textbf{Input}: 2 input layers, one for the real image and one for the generated image.
    \item \textbf{Concatenate}: the two input layers are concatenated along the channel axis.
    \item \textbf{Downsampling}: the concatenated input is downsampled using 3 convolutional layers, each convolutional layer is composed by a convolutional layer, a batch normalization layer, and a Leaky ReLU activation layer.
    \item \textbf{Output}: the output layer is a 30$\times$30$\times$1 matrix, where each patch of the output classifies a 70$\times$70 portion of the input image as real or fake.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{model/discriminator-layer}
    \caption{TensorFlow implementation of the PatchGAN discriminator}\label{fig:dis-layer}
\end{figure}
\subsection{Adam optimizer}\label{subsubsec:adam-optimizer}
The Adam optimizer is a widely used optimization algorithm for training neural networks. 
It was introduced by Diederik P. Kingma and Jimmy Ba in their paper titled ``Adam: A Method for Stochastic Optimization'~\footcite{paper:kingma2014adam} published in 2015. 
Adam stands for Adaptive Moment Estimation and combines the benefits of two other optimization techniques: AdaGrad and RMSProp. It maintains adaptive learning rates for each parameter, automatically adjusting the learning rate based on the gradient's past behavior. 
By utilizing first and second moments of the gradients, Adam updates the parameters to accelerate convergence and handle different types of neural networks effectively.
It can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based on training data.
According to Kingma et al., the method is ``computationally efficient, has little memory requirements, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data and/or parameters``.
\par
Following the pix2pix paper, the Adam optimizer is used with a learning rate of 0.0002 and momentum of 0.5 for both the generator and the discriminator.
\subsection{Training process}\label{subsec:training-process}
For the generator training the procedure is the following illustrated in fig.~\ref{fig:gen-training}, Meanwhile for the discriminator training the procedure is the following illustrated in fig.~\ref{fig:dis-training}.
\par
The training process begins with pairs of input images and their corresponding target output images. 
The generator network takes the input image as input and generates a synthesized output image. 
The discriminator network, on the other hand, receives both the synthesized output image from the generator and the real target output image. 
The discriminator's objective is to correctly classify whether the input image is real or synthesized.
\par
The training process involves alternating between two steps: generator update and discriminator update. 
In the generator update step, the generator parameters are updated to minimize the discrepancy between the synthesized output image and the target output image. 
This is typically done by minimizing a pixel-wise loss function, such as mean squared error or binary cross-entropy, which measures the difference between the synthesized and target images.
\par 
In the discriminator update step, the discriminator parameters are updated to improve its ability to discriminate between real and synthesized images. 
The discriminator is trained to correctly classify real images as real and synthesized images as fake. It aims to maximize its classification accuracy.
\par
The training process continues iteratively, with the generator and discriminator networks playing a competitive game. 
The generator learns to generate more realistic and visually appealing output images that closely resemble the target images, while the discriminator becomes more skilled at distinguishing between real and synthesized images.
\par
This adversarial training process creates a feedback loop where the generator tries to produce images that the discriminator cannot distinguish from real ones, and the discriminator continuously improves its ability to discriminate between real and synthesized images. 
This iterative training process helps the generator network learn to generate high-quality output images that are visually consistent with the target images.
\par
The training process of the pix2pix generator and discriminator involves this iterative interplay, gradually improving the generator's ability to synthesize realistic output images and the discriminator's ability to distinguish between real and synthesized images.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{model/gen-training}
    \caption{Generator training process}\label{fig:gen-training}
    \includegraphics[width=0.50\textwidth]{model/dis-training}
    \caption{Discriminator training process}\label{fig:dis-training}
\end{figure}
\subsection{User Interface}\label{subsec:user-interface}
As a requirements of the project, a user interface (ref~\ref{fig:ui}) was developed to allow the user to draw the input image.
It was developed using the Python library Tkinter and it is composed by a window with a menu bar and a canvas where the user can draw.
The menu bar consists of the following:
\begin{itemize}
    \item \textbf{Colors}
    \begin{itemize}
        \item \textbf{Brush Color}: Set the Brush color.
        \item \textbf{background Color}: Set the background color.
    \end{itemize}
    \item \textbf{Options}
    \begin{itemize}
        \item \textbf{Clear canvas}: Undo the last action.
        \item \textbf{Generate Image}: Redo the last action.
        \item \textbf{Load CAD}: Load a CAD file as input.
        \item \textbf{Load PNG/JPG}: Load a .PNG file as input.
        \item \textbf{Exit}: Exit the application.
    \end{itemize}
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{ui/ui}
    \caption{User interface}\label{fig:ui}
\end{figure}
Once the input is submitted, the application will show the output in the lower canvas.
And save it in the \textit{output} folder.

\chapter{Process and methodologies}
\label{cap:process-methodologies}

\intro{Brevissima introduzione al capitolo}\\

\section{Evalutation of a GAN model}
\label{sec:evalutation-gan-model}
\subsection{The problem of evaluating GANs}
\label{subsec:problem-evaluating-gans}
Unlike other deep learning models, that are trained with a loss function until convergence, GANs are trained in a zero-sum game between two networks, the generator and the discriminator. 
The generator tries to fool the discriminator, while the discriminator tries to distinguish between real and fake samples. 
The training process is considered complete when the discriminator can no longer distinguish between real and fake samples. 
The generator is then considered to have learned the underlying distribution of the training data.\\
Thus, there is no objective function to minimize or maximize, and no way to objectively evaluate training progress and realtive or absolute performance of a GAN model from loss alone.
This is a major problem for GANs, when:
\begin{itemize}
    \item choosing a final GAN model during a training run;
    \item choosing generated samples to demonstrate the capabilities of a GAN model;
    \item comparing different GAN models;
    \item comparing different hyperparameters for the same GAN model;
\end{itemize}
So far, the most common way to evaluate GANs is to use a combination of qualitative and quantitative metrics based on the quality and diversity of the generated samples.
\subsection{Manual evaluation}
\label{subsec:manual-evaluation}
Many times the evaluation of a GAN model is done manually, by visual inspection of the generated samples.
This involves the use of human judgement to evaluate the quality of a batch of generated samples, and is therefore subjective.
Manual inspections is the simplest method of model evaluation, but it has many drawbacks:
\begin{itemize}
    \item it is subjective, including biases of the human evaluator about the model and the data;
    \item it require knowledge of the domain of the data;
    \item it is time consuming, so limited to the number of images that can be evaluated;
\end{itemize}
\emph{…evaluating the quality of generated images with human vision is expensive and cumbersome, biased […] difficult to reproduce, and does not fully reflect the capacity of models.}
\footcite{paper:ganeval}\\
This mean that this process will be for sure biased, so it shouldn't be used for a final model selection,
 but it can be used to get a first impression of the model performance.
Thankfully, more objective methods of evaluation have been proposed and adopted.
\subsection{Qualitative evaluation}
\label{subsec:qualitative-evaluation}
Qualitative measures are based on the visual inspection of the generated samples, but they are more objective than manual evaluation.
Some of the most common qualitative measures are:
\begin{itemize}
    \item \textbf{Nearest neighbors}: used to detect overfitting, by comparing the generated samples with the training data;
    \item \textbf{Rapid Scene Categorization}\footcite{paper:rapidscenecat}:in these experiments, subjects were asked to categorize images as real or fake as quickly as possible;
    \item \textbf{Rating and Preference judgement}\footcite{paper:stackadvnet}\footcite{paper:stackadvnet1}: 
    partecipants are asked to rate the quality of the generated samples, or to choose the best samples from a set of generated samples;
    \item Evaluating Mode Drop and Mode Collaps;
    \item Investigating and visualizing the internals of networks;
\end{itemize}

\subsection{Quantitative evaluation}
\label{subsec:quantitative-evaluation}
Quantitative measures are based on the use of metrics to evaluate the quality of the generated samples.
All this methods use a numerical score to evaluate the quality.
Some of the most common quantitative measures are:
\begin{itemize}
    \item Average log-likelihood;
    \item Coverage Metric;
    \item Inception Score;
    \item Modified Inception Score;
    \item Mode Score;
    \item AM Score;
    \item Frechet Inception Distance;
    \item Maximum Mean Discrepancy;
    \item The Wasserstein Distance;
    \item Birthday Paradox Test;
    \item Classifier Two-Sample Tests;
    \item Classification Performance
    \item Boundary Distortion 
    \item Number of Statistically-Different Bins
    \item Image Retrieval Performance
    \item Generative Adversarial Metric
    \item Tournament Win Rate
    \item Normalized Relative Discriminative Score
    \item Adversarial Accuracy and Adversarial Divergence
    \item Geometric Score
    \item Reconstruction Score
    \item Image Quality measures
    \item Low-level Image Statistics
    \item Precision, Recall and F1 Score
\end{itemize}
\chapter{Process and methodologies}
\label{cap:process-methodologies}

\intro{Brevissima introduzione al capitolo}\\

\section{Evalutation of a GAN model}
\label{sec:evalutation-gan-model}
\subsection{The problem of evaluating GANs}
\label{subsec:problem-evaluating-gans}
Unlike other deep learning models, that are trained with a loss function until convergence, GANs are trained in a zero-sum game between two networks, the generator and the discriminator. 
The generator tries to fool the discriminator, while the discriminator tries to distinguish between real and fake samples. 
The training process is considered complete when the discriminator can no longer distinguish between real and fake samples. 
The generator is then considered to have learned the underlying distribution of the training data.\\
Thus, there is no objective function to minimize or maximize, and no way to objectively evaluate training progress and realtive or absolute performance of a GAN model from loss alone.
This is a major problem for GANs, when:
\begin{itemize}
    \item choosing a final GAN model during a training run;
    \item choosing generated samples to demonstrate the capabilities of a GAN model;
    \item comparing different GAN models;
    \item comparing different hyperparameters for the same GAN model;
\end{itemize}
So far, the most common way to evaluate GANs is to use a combination of qualitative and quantitative metrics based on the quality and diversity of the generated samples.
\subsection{Manual evaluation}
\label{subsec:manual-evaluation}
Many times the evaluation of a GAN model is done manually, by visual inspection of the generated samples.
This involves the use of human judgement to evaluate the quality of a batch of generated samples, and is therefore subjective.
Manual inspections is the simplest method of model evaluation, but it has many drawbacks:
\begin{itemize}
    \item it is subjective, including biases of the human evaluator about the model and the data;
    \item it require knowledge of the domain of the data;
    \item it is time consuming, so limited to the number of images that can be evaluated;
\end{itemize}
\emph{…evaluating the quality of generated images with human vision is expensive and cumbersome, biased […] difficult to reproduce, and does not fully reflect the capacity of models}
\footcite{paper:ganeval}.\\\\
This mean that this process will be for sure biased, so it shouldn't be used for a final model selection,
 but it can be used to get a first impression of the model performance.
Thankfully, more objective methods of evaluation have been proposed and adopted.
\subsection{Qualitative evaluation}
\label{subsec:qualitative-evaluation}
Qualitative measures are based on the visual inspection of the generated samples, but they are more objective than manual evaluation.
Some of the most common qualitative measures are:
\begin{itemize}
    \item \textbf{Nearest neighbors}: used to detect overfitting, by comparing the generated samples with the training data;
    \item \textbf{Rapid Scene Categorization}\footcite{paper:rapidscenecat}:in these experiments, subjects were asked to categorize images as real or fake as quickly as possible;
    \item \textbf{Rating and Preference judgement}\footcite{paper:stackadvnet}\footcite{paper:stackadvnet1}\footcite{paper:stackadvnet2}\footcite{paper:stackadvnet3}: 
    partecipants are asked to rate the quality of the generated samples, or to choose the best samples from a set of generated samples;
    \item \textbf{Mode Drop and Collapse}\footcite{paper:dropandcollapse}\footcite{paper:dropandcollapse2}: over datasets with multiple known modes, modes are coumputed as by measuring the distance of generated data to mode centers;
    \item \textbf{Network Internals}\footcite{paper:netint}\footcite{paper:netint1}\footcite{paper:netint2}\footcite{paper:netint3}\footcite{paper:netint4}\footcite{paper:netint5}: 
    the internal state of the models is used to evaluate the quality of the generated samples (e.g. space continuity, neuron activation, etc.);
\end{itemize}
The most used qualitative measure is a sort of manual inspection of images, called \emph{Rating and Preference judgement}.\\\\
\emph{These types of experiments ask subjects to rate models in terms of the fidelity of their generated images}\footcite{paper:ganeval}.\\\\
Usually images are shown in pairs (one real and one fake), and the subjects are asked to choose the best image.
A score or rating is then assigned to the model based on the number of times it is chosen as the best image.
For lowering the variance of the results, the images are shown to multiple human judges and the results are averaged.
This process is labor intensive, but with the help of crowdsourcing platforms like Amazon Mechanical Turk, 
it can be done at scale (Reducing the cost also).\\
Another downside of this method is that the human judges performance can improve with experience, especially if they are given feedback on their performance.\\\\
\emph{By learning from such feedback, annotators are better able to point out the flaws in generated images, giving a more pessimistic quality assessment}\footcite{paper:ganeval}.\\\\

Another similar method for subjectively evaluating the quality of generated images is \emph{"Nearest neighbors"}.
In this method, some examples of real images are picked from the domain of the data, and than it's picked the most similar generated images for comparison.
Some of the distance measures used for this method are Euclidean distance, cosine distance, and L1 distance.
Nearest neighbors can be used to determine how realistic the generated images are.


\subsection{Quantitative evaluation}
\label{subsec:quantitative-evaluation}
Quantitative measures are based on the use of metrics to evaluate the quality of the generated samples.
All this methods use a numerical score to evaluate the quality.
Some of the most common quantitative measures are:
\begin{itemize}
    \item Average log-likelihood;
    \item Coverage Metric;
    \item Inception Score;
    \item Modified Inception Score;
    \item Mode Score;
    \item AM Score;
    \item Frechet Inception Distance;
    \item Maximum Mean Discrepancy;
    \item The Wasserstein Distance;
    \item Birthday Paradox Test;
    \item Classifier Two-Sample Tests;
    \item Classification Performance
    \item Boundary Distortion 
    \item Number of Statistically-Different Bins
    \item Image Retrieval Performance
    \item Generative Adversarial Metric
    \item Tournament Win Rate
    \item Normalized Relative Discriminative Score
    \item Adversarial Accuracy and Adversarial Divergence
    \item Geometric Score
    \item Reconstruction Score
    \item Image Quality measures
    \item Low-level Image Statistics
    \item Precision, Recall and F1 Score
\end{itemize}

The most used quantitative measures are \emph{Inception Score} and \emph{Frechet Inception Distance}.
\subsubsection{Inception Score}
\label{subsubsec:inception-score}
Inception Score (IS) is a quantitative measure of the quality of generated images based on the Inception model.
It was proposed in 2016 by Tim Salimans et al. in \emph{"Improved Techniques for Training GANs"}\footcite{paper:salimans2016improved}.
The Inception Score is based on the idea that a good generative model should produce samples that are both \emph{diverse} and \emph{realistic}.
Calculating the IS requires the use of a pretrained Inception model, the Inception v3 model\footcite{paper:inceptionv3} and a dataset of generated images.
The Inception model is used to compute the conditional label distribution $p(y|x)$, where $x$ is an image and $y$ is a class label.
The Inception Score is then computed as:
\begin{equation}
    \label{eq:inception-score}
    \exp \left( \mathbb{E}_{x \sim p_{g}} \left[ \mathbb{KL}(p(y|x) || p(y)) \right] \right)
\end{equation}
where $p(y)$ is the marginal label distribution, and $p_{g}$ is the distribution of generated images.
The marginal label distribution $p(y)$ is approximated by computing the class distribution of the training set.
The Inception Score is then computed as the exponential of the KL divergence between the conditional label distribution and the marginal label distribution.
The KL divergence is a measure of how one probability distribution is different from a second, reference probability distribution.
The KL divergence is defined as:
\begin{equation}
    \label{eq:kl-divergence}
    \mathbb{KL}(p(y|x) || p(y)) = \sum_{y} p(y|x) \log \frac{p(y|x)}{p(y)}
\end{equation}
The Inception Score is a good measure of the quality of generated images, but it has some limitations.
The Inception Score is not a good measure of the diversity of generated images, and it is not a good measure of the quality of generated images when the dataset contains multiple modes.
\subsubsection{Frechet Inception Distance}
\label{subsubsec:frechet-inception-distance}
Frechet Inception Distance (FID) is a quantitative measure of the quality of generated images based on the Inception model.
It was proposed in 2017 by Martin Heusel et al. in \emph{"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"}\footcite{paper:heusel2017gans}.\\\\
\emph{"FID performs well in terms of discriminability, robustness and computational efficiency [...] It has been shown that FID is consistent with human judgments and is more robust to noise than IS".}\footcite{paper:ganeval}\\\\
Calculating the FID requires the use of a pretrained Inception model, the Inception v3 model\footcite{paper:inceptionv3} like the Inception Score.
The FID is computed as the Frechet distance between the feature representations of the real images and the generated images.
The Frechet distance is a measure of the similarity between two probability distributions.
The FID is defined as:
\begin{equation}
    \label{eq:frechet-inception-distance}
    \text{FID}(p_{r}, p_{g}) = \left\| \mu_{r} - \mu_{g} \right\|_{2}^{2} + \text{Tr} \left( \Sigma_{r} + \Sigma_{g} - 2 \left( \Sigma_{r} \Sigma_{g} \right)^{1/2} \right)
\end{equation}
where $\mu_{r}$ and $\mu_{g}$ are the mean vectors of the real and generated images, and $\Sigma_{r}$ and $\Sigma_{g}$ are the covariance matrices of the real and generated images.
The FID is a good measure of the quality of generated images, but it has some limitations.
The FID is not a good measure of the diversity of generated images, and it is not a good measure of the quality of generated images when the dataset contains multiple modes.
\subsubsection{Suggested GAN evaluation procedure}
\label{subsubsec:suggested-gan-evaluation-procedure}
At the starting of this section, we have seen that the evaluation of GANs is a difficult task.
A good way to evaluate GANs is to use a combination of qualitative and quantitative measures.
Starting with a manual inspection of the generated images, in order to evaluate the quality of the generator model.
After that, we can use some quantitative measures to evaluate the quality of the generated images and the diversity like the Inception Score and the Frechet Inception Distance.
Remember, that there is no single best measure for evaluating GANs, and that the choice of the evaluation measures depends on the task and the dataset.\\\\

\emph{
    As of yet, there is no consensus regarding the best score. Different scores assess various aspects of the image generation process, and it is unlikely that a single score can cover all aspects. Nevertheless, some measures seem more plausible than others (e.g. FID score)
}\footcite{paper:ganeval}\\\\